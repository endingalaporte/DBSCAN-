{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "deadly-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import graph_tool as gt\n",
    "import csv\n",
    "from sklearn.decomposition import PCA\n",
    "import time \n",
    "import itertools\n",
    "\n",
    "folder_path = \"/Users/endingalaporte/Desktop/graphlet_degree_kernel\"\n",
    "input_path = folder_path + \"/input\"\n",
    "output_path = folder_path + \"/output\"\n",
    "\n",
    "\n",
    "#core\n",
    "\n",
    "def x9_counts(a : list,b : list,c):\n",
    "    \"\"\"given a b c respectively lists Su, Sv and count x7 integer, returns integer count x9\"\"\"\n",
    "    if len(a)>=2 and len(b) >= 2:\n",
    "        x9_count = k_parmi_n(2, len(a)) + k_parmi_n(2, len(b)) - c\n",
    "    if len(a)<2 or len(b) <2:\n",
    "        x9_count = 0\n",
    "    if x9_count <0:\n",
    "        x9_count = 0\n",
    "    return x9_count\n",
    "    \n",
    "def x6_count(a : list, b):\n",
    "        \"\"\"given a and b, where a is list Te and b is count integer x5, returns counts x6\"\"\"\n",
    "        if len(a) >= 2:\n",
    "            x6_count = k_parmi_n(2, len(tuple(a))) - b\n",
    "        if len(a) < 2:\n",
    "            x6_count = 0\n",
    "        return x6_count\n",
    "    \n",
    "def update_vertices_including(old_array : np.ndarray, v1 : gt.libgraph_tool_core.Vertex):\n",
    "    \"\"\"given array and vertex object, return array including one time the vertex\"\"\"\n",
    "    if v1 not in old_array:\n",
    "        new_array = np.append(old_array, v1)\n",
    "    if v1 in old_array:\n",
    "        new_array = old_array\n",
    "    return(new_array)\n",
    "\n",
    "def update_vertices_without(old_array : np.ndarray, v1 : gt.libgraph_tool_core.Vertex):\n",
    "    \"\"\"given array and vertex object, returns array without the object\"\"\"\n",
    "    new_array = old_array[old_array != v1]\n",
    "    return(new_array)\n",
    "\n",
    "def k_parmi_n(k,n):\n",
    "    \"\"\"binomial coefficient k parmi n\"\"\"\n",
    "    num = np.math.factorial(n)\n",
    "    denom = np.math.factorial(k)*np.math.factorial(n-k)\n",
    "    return num/denom\n",
    "\n",
    "def d(w):\n",
    "    \"\"\"given vertex w, returns its total degree\"\"\"\n",
    "    return (w.in_degree() + w.out_degree())\n",
    "\n",
    "def S_set(e : gt.libgraph_tool_core.Edge, w : gt.libgraph_tool_core.Vertex):\n",
    "    \"\"\"given edge e and w vertex, gives set Sw\"\"\"\n",
    "    Sw = []\n",
    "    v = e.source()\n",
    "    u = e.target()\n",
    "    neigh_w = list(w.all_neighbors())\n",
    "    if w == v: #Sv\n",
    "        neigh_w.remove(u)\n",
    "    if w == u: #Su\n",
    "        neigh_w.remove(v)\n",
    "    for w in neigh_w:\n",
    "        Sw.append(w)\n",
    "    return Sw\n",
    "\n",
    "def Te_set(e : gt.libgraph_tool_core.Edge):\n",
    "    \"\"\"given graph G and edge e, returns set of vertices Te that form triangles with edge e\"\"\"\n",
    "    v = e.source()\n",
    "    u = e.target()\n",
    "    Te = []\n",
    "    \n",
    "    #create iterator over all vertex neighbors of v and u, remove repeated vertex, remove v and u\n",
    "    ite_without_repeat = list(itertools.chain(v.all_neighbors(), u.all_neighbors()))\n",
    "    for w in v.all_neighbors():\n",
    "        if w in u.all_neighbors():\n",
    "            ite_without_repeat.remove(w)\n",
    "    ite_without_repeat.remove(u) #should remove u and v from this, otherwise we test\n",
    "    ite_without_repeat.remove(v) #if u is in u and it is, but can be triangle with itself\n",
    "    \n",
    "    #create Te set of vertices following math definition of the set Te    \n",
    "    for w in ite_without_repeat:\n",
    "        if (w in v.all_neighbors()) and (w in u.all_neighbors()):\n",
    "            Te.append(w)\n",
    "    return(Te)\n",
    "\n",
    "def LOCALGRAPHLET(G : gt.Graph, e : gt.libgraph_tool_core.Edge):\n",
    "    \"\"\"given graph G and edge e, returns graphlet G1,G2,G5,G6,G7,G8,G9,G10 degree vector of the edge\"\"\"\n",
    "    #graphlet counts\n",
    "    x5 = 0\n",
    "    x7 = 0\n",
    "    x8 = 0\n",
    "    sigma = 0\n",
    "    \n",
    "    #lookup table is a dictionnary\n",
    "    psi = dict()\n",
    "    \n",
    "    #vertices objects and sets of vertices objects e = (v, u) #not (u, v) order matters\n",
    "    v = e.source()\n",
    "    u = e.target()\n",
    "    Sv = S_set(e, v)\n",
    "    Su = S_set(e, u)\n",
    "    \n",
    "    #set of edge objects\n",
    "    Te = Te_set(e) \n",
    "    \n",
    "    avant = len(Te)\n",
    "    #count graphlets x5, x7, x8 directly in graph\n",
    "    for w in v.all_neighbors():\n",
    "        if w != u: #line 3\n",
    "            Sv = update_vertices_including(np.array(Sv, dtype = gt.libgraph_tool_core.Vertex), w).tolist()\n",
    "            psi[w] = 'lambda1'\n",
    "    for w in u.all_neighbors():\n",
    "        if w != v: #line 5\n",
    "            if (w in psi.keys()) == True: \n",
    "                if psi[w] == 'lambda1': #careful to check w is a key of dictionnary psi before calling value psi[w]\n",
    "                    Te = update_vertices_including(np.array(Te, dtype = gt.libgraph_tool_core.Vertex), w).tolist() \n",
    "                    psi[w] = 'lambda3'   \n",
    "                    Sv = update_vertices_without(np.array(Sv, dtype = gt.libgraph_tool_core.Vertex), w).tolist()\n",
    "            if ((w in psi.keys()) == True and psi[w] != 'lambda') or (w not in psi.keys()):  \n",
    "                Su = update_vertices_including(np.array(Su, dtype = gt.libgraph_tool_core.Vertex), w).tolist()\n",
    "                psi[w] = 'lambda2'\n",
    "    for w in Te:\n",
    "        for r in w.all_neighbors():\n",
    "            if (r in psi.keys()) == True:\n",
    "                if psi[r] == 'lambda3':\n",
    "                    x5 = x5 +1\n",
    "                psi[w] = 'lambda4'\n",
    "    for w in Su:\n",
    "        for r in w.all_neighbors():\n",
    "            if (r in psi.keys()) == True:\n",
    "                if psi[r] == 'lambda1':\n",
    "                    x8 = x8 + 1\n",
    "                if psi[r] == 'lambda2':\n",
    "                    x7 = x7 + 1\n",
    "                if psi[r] == 'lambda4':\n",
    "                    sigma = sigma + 1\n",
    "        psi[w] = 0\n",
    "    for w in Sv:\n",
    "        for r in w.all_neighbors():\n",
    "            if (r in psi.keys()) == True: #is it necessary to check if r in psi.keys() here? Did we did it just before?\n",
    "                if psi[r] == 'lambda1':\n",
    "                    x7 = x7 + 1\n",
    "                if psi[r] == 'lambda4':\n",
    "                    sigma = sigma + 1\n",
    "        psi[w] = 0\n",
    "    \n",
    "    #graphlets counts x1, x2, x6, x9, x10 deduced\n",
    "    \n",
    "    x1 = len(Te)\n",
    "    x2 = (d(u) + d(v) - 2) - 2*len(Te) #sometimes get negative counts\n",
    "    #x2 = len(Su) + len(Sv)\n",
    "    x6 = x6_count(Te, x5)\n",
    "    #delete that line if x6 works without x6 = k_parmi_n(2, len(tuple(Te))) - x5 #need to solve, (k parmi n) for n >=k, otherwise negative factorial is impossible\n",
    "    x9 = x9_counts(Su, Sv, x7)\n",
    "    #delete that line if x6 works without x9 x9 = k_parmi_n(2, len(Sv)) + k_parmi_n(2, len(Su)) - x7\n",
    "    x10 = (len(Sv)*len(Su)) - x8\n",
    "    \n",
    "    after = len(Te)\n",
    "    return(np.array([x1, x2, x5, x6, x7, x8, x9, x10], dtype = int))\n",
    "                     #'Te',len(Te),'Sv',len(Sv),'Su',len(Su),\"Te avant\",avant,\"et after\",after], dtype = object))\n",
    "\n",
    "def matrix_count(G : gt.Graph, draw):\n",
    "    \"given graph G object, draw G and returns matrix of graph degree for each edge\"\n",
    "    if draw == 'yes':\n",
    "        size = 120*int(math.log(1 + len(tuple(G.edges()))))\n",
    "        gtd.graph_draw(G, edge_text = G.edge_index, edge_font_size = 12, vertex_text=G.vertex_index, vertex_font_size=10, output_size = (size,size))\n",
    "    if draw == 'no':\n",
    "        print(G)\n",
    "    s = time.time()\n",
    "    matrix = []\n",
    "    for e in G.edges():\n",
    "        matrix.append(LOCALGRAPHLET(G, e))\n",
    "    matrix = np.array(matrix)\n",
    "    print(\"duration to compute matrix is =\",time.time()-s)\n",
    "    return matrix\n",
    "\n",
    "\n",
    "#kernel \n",
    "def remove_column(entry):\n",
    "    \"\"\"given array, returns the array without the columns filled with zeros\"\"\"\n",
    "    \n",
    "    def delete_column(array, i):\n",
    "        \"\"\"\n",
    "        given array and i-th column index\n",
    "        return the array without that i-th column\n",
    "        \"\"\"\n",
    "        array = np.delete(array, i, 1)\n",
    "        return array\n",
    "    \n",
    "    def test_zero(array):\n",
    "        \"\"\"given array, return True if it contains only zeros\"\"\"\n",
    "        return np.all(array == 0)\n",
    "    \n",
    "    array = []\n",
    "    for c in range(entry.shape[1]):\n",
    "        if test_zero(entry[:,c]) == False:\n",
    "            array.append(entry[:,c])\n",
    "    array = np.array(array).T\n",
    "    return array\n",
    "\n",
    "def hist_d(array : np.ndarray, d):\n",
    "    \"\"\"\n",
    "    given array of integer values,\n",
    "    returns the histogram of consecutive integer values without hole\n",
    "    and fill with zeros until \"sortie\" array is of length d\n",
    "    \"\"\"\n",
    "    bins = np.append(np.arange(0,array.max()+1)-0.5,array.max()+0.5)\n",
    "    le = len(np.histogram(array, bins = bins)[0])\n",
    "    h = np.histogram(array, bins = bins)[0][1:le]\n",
    "    sortie = np.zeros(d)\n",
    "    l = len(h)\n",
    "    sortie[0:l] = h[0:d]\n",
    "    return sortie\n",
    "\n",
    "def string_to_list(string):\n",
    "    \"\"\"given a string of two elements, returns each element in a list\"\"\"\n",
    "    return [int(string.split(' ')[0]), int(string.split(' ')[1])]\n",
    "\n",
    "def Q_phi(Q : np.ndarray, d_gc):\n",
    "    \"\"\"\n",
    "    given matrix of counts Q and histogram length, \n",
    "    returns the phi embedding \n",
    "    of consecutive histogram of each graphlet degree\n",
    "    \"\"\"\n",
    "    phi = []\n",
    "    for i in range(Q.shape[1]):\n",
    "        h = hist_d(Q[:,i], d_gc)\n",
    "        \n",
    "        if h.sum() != 0:\n",
    "            h = h/h.sum()\n",
    "            phi.append(h)\n",
    "        if h.sum() == 0:\n",
    "            phi.append(np.zeros(d_gc))\n",
    "    phi= np.concatenate(np.array(phi))\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import labels\n",
    "\n",
    "n = 136 #nb of cities\n",
    "\n",
    "label_path = input_path + '/country_binary.txt'\n",
    "text = open(label_path, encoding = 'utf-16')\n",
    "reader = csv.reader(text, delimiter='\\n')\n",
    "labels = np.array([int(row[0]) for row in reader])\n",
    "\n",
    "#import graphs\n",
    "print(\"step 1\")\n",
    "s = time.time()\n",
    "G_list = np.array([gt.Graph(directed = False) for i in range(n)], dtype = object)\n",
    "o_f = input_path\n",
    "for i in range(n):\n",
    "    open_path = o_f + \"/edgelist_city\"+str(i)+\".txt\"\n",
    "    #print(open_path)\n",
    "    text = open(open_path, 'r')\n",
    "    reader = csv.reader(text, delimiter='\\n')\n",
    "    elist = [string_to_list(row[0]) for row in reader]\n",
    "    G_list[i].add_edge_list(elist)   \n",
    "print(len(G_list),\"cities imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-appraisal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-doctrine",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed each graph into matrix of count Q\n",
    "print(\"step 2, t=\", time.time()-s)\n",
    "s = time.time()\n",
    "all_Q = []\n",
    "b = 0\n",
    "\n",
    "for g in G_list:\n",
    "    all_Q.append(matrix_count(g, draw = 'no'))\n",
    "    print(b, \"-135\")\n",
    "    b = b +1\n",
    "    \n",
    "all_Q = np.array(all_Q)\n",
    "print(time.time()-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "3512.6508519649506/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiovascular-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "storage = all_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_storage = all_Q #will be usefull for ploting choice 2 not standardized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-ultimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-ceremony",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#transform each Q into phi\n",
    "print(\"step 3, t=\", time.time()-s)\n",
    "\n",
    "d_gc = np.concatenate(all_Q).max()\n",
    "print(\"max count accross all cities is d_gc =\",d_gc)\n",
    "\n",
    "all_phi = []\n",
    "for q in all_Q:\n",
    "    all_phi.append(Q_phi(q, d_gc))\n",
    "    \n",
    "all_phi = np.array(all_phi)\n",
    "\n",
    "#save it in case\n",
    "np.savetxt(output_path + \"/all_phi.txt\",all_phi)\n",
    "print(\"before removing columns of zeros, all_phi of shape\",all_phi.shape)\n",
    "all_phi = remove_column(all_phi)\n",
    "print(\"after, shape is \",all_phi.shape)\n",
    "\n",
    "# Choice 1 'standardize'\n",
    "# choice 2 'non standardize'\n",
    "\n",
    "choice = 'standardize'\n",
    "\n",
    "if choice == 'standardize':\n",
    "    for i in range(len(all_phi)):\n",
    "        if all_phi[i].std !=0:\n",
    "            all_phi[i] =  (all_phi[i]- all_phi[i].mean())/ all_phi[i].std()\n",
    "        if all_phi[i].std == 0:\n",
    "            all_phi[i] =  all_phi[i]- all_phi[i].mean()\n",
    "\n",
    "#visualize each phi\n",
    "print(\"step 4, t = \", time.time()-s)\n",
    "\n",
    "plt.figure(figsize = (10,10), dpi = 200)\n",
    "for each in all_phi:\n",
    "    plt.plot(each, linewidth=0.5)\n",
    "plt.savefig(output_path+\"/graphlet_degree_normalized_standardized_allphi.svg\")\n",
    "\n",
    "\n",
    "#count the number c of std = 0 accross each column of all_phi\n",
    "all_phi = all_phi\n",
    "\n",
    "all_phi.shape[1]\n",
    "\n",
    "std = []\n",
    "for j in range(all_phi.shape[1]):\n",
    "    std.append(all_phi[:,j].std())\n",
    "std = np.array(std)\n",
    "\n",
    "c = 0\n",
    "for i in std == 0:\n",
    "    if i == True:\n",
    "        c = c+1\n",
    "print(c,\"column(s) of all_phi with std = 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-effects",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel matrix\n",
    "K = np.zeros([n,n])\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        K[i,j] = np.dot(all_phi[i], all_phi[j])\n",
    "\n",
    "#PCA\n",
    "\n",
    "reducer = PCA(n_components = 3)\n",
    "embed = reducer.fit_transform(K)\n",
    "plt.figure(figsize = (5,5), dpi = 200)\n",
    "plt.scatter(embed[:,0], embed[:,1], c = labels, facecolors='none')\n",
    "plt.savefig(output_path + 'graphlet_degree_normalized_standardized_PCA01.svg')\n",
    "plt.figure(figsize = (5,5), dpi = 200)\n",
    "plt.scatter(embed[:,1], embed[:,2], c = labels,facecolors='none')\n",
    "plt.savefig(output_path + 'graphlet_degree_normalized_standardized_PCA12.svg')\n",
    "plt.figure(figsize = (5,5), dpi = 200)\n",
    "plt.scatter(embed[:,2], embed[:,0], c = labels, facecolors='none')\n",
    "plt.savefig(output_path + 'graphlet_degree_normalized_standardized_PCA20.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-valuable",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array(labels, dtype = object)\n",
    "for i in range(len(labels)):\n",
    "    if labels[i] == 0:\n",
    "        labels[i] = 'C0'\n",
    "    if labels[i] == 1:\n",
    "        labels[i] = 'C1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-comment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plots of three components of PCA\n",
    "\n",
    "#%matplotlib #interactive plot\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "plt.ion()\n",
    "\n",
    "x, y, z = embed[:,0], embed[:,1], embed[:,2]\n",
    "\n",
    "fig = plt.figure() #figsize = (8,6), dpi = 300\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "zline = z\n",
    "xline = x\n",
    "yline = y\n",
    "angle = 120\n",
    "ax.view_init(20, angle)\n",
    "ax.set_xlabel('1st component')\n",
    "ax.set_ylabel('2nd component')\n",
    "ax.set_zlabel('3rd component')\n",
    "ax.scatter(xline, yline, zline, c = labels, s = 30)\n",
    "plt.savefig(output_path + 'graphlet_degree_normalized_standardized_3D_PCA.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "champion-recommendation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,8), dpi = 200)\n",
    "plt.plot(all_phi[75], linewidth = 0.5)\n",
    "plt.savefig(output_path + 'graphlet_degree_normalized_standardized_paris_phi.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-ranking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "silhouette_score(embed, labels), silhouette_score(all_phi, labels)\n",
    "#(0.13715280276833852, 0.23220126856059697) normalized graphlet degree kernel\n",
    "#(0.2349476268757523, 0.2456655234540093) normalized standardized graphlet degree kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(all_phi, labels, metric = 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-student",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "X = all_phi\n",
    "plt.scatter(embed[:,2], embed[:,0], c = labels)\n",
    "plt.show()\n",
    "D = pairwise_distances(all_phi, metric = 'l1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize clustering DBSCAN heuristics\n",
    "min_samples = 3\n",
    "epsilon = 1.6\n",
    "#min_samples = 3\n",
    "#epsilon = 1.6 #3 clusters\n",
    "\n",
    "#min_samples = 5\n",
    "#epsilon = 1.91 #2clusters\n",
    "clustering = DBSCAN(eps= epsilon, min_samples=min_samples, metric = 'precomputed').fit(D)\n",
    "plt.subplots()\n",
    "title = \"min_sample =\"+str(min_samples)+\", eps =\"+str(epsilon)\n",
    "#print(title)\n",
    "plt.scatter(embed[:,0], embed[:,1], c = clustering.labels_)\n",
    "#plt.savefig(\"/Users/endingalaporte/Desktop/3clusters_DBSCAN_PCA01.svg\")\n",
    "plt.subplots()\n",
    "plt.scatter(embed[:,1], embed[:,2], c = clustering.labels_)\n",
    "#plt.savefig(\"/Users/endingalaporte/Desktop/3clusters_DBSCAN_PCA12.svg\")\n",
    "plt.subplots()\n",
    "plt.scatter(embed[:,2], embed[:,0], c = clustering.labels_)\n",
    "#plt.savefig(\"/Users/endingalaporte/Desktop/3clusters_DBSCAN_PCA20.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete noise points\n",
    "embed.shape, clustering.labels_.shape \n",
    "partition = []\n",
    "partition_label = []\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    if clustering.labels_[i] != -1:\n",
    "        partition.append(embed[i])\n",
    "        partition_label.append(clustering.labels_[i])\n",
    "partition = np.array(partition)\n",
    "partition_label = np.array(partition_label)\n",
    "\n",
    "print(embed.shape[0]-partition.shape[0],\"datapoints belonging to noise cluster removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#centroid for each cluster\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "\n",
    "X = partition\n",
    "y = partition_label\n",
    "clf = NearestCentroid()\n",
    "clf.fit(X, y)\n",
    "centroids = clf.centroids_\n",
    "centroids_label = np.array(['T1', 'T2', 'T3'])\n",
    "plt.scatter(partition[:,2], partition[:,0], c = partition_label)\n",
    "plt.scatter(centroids[:,2], centroids[:,0], c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behavioral-tunnel",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3D plots of points\n",
    "\n",
    "x, y, z = partition[:,0], partition[:,1], partition[:,2]\n",
    "\n",
    "partition_label = np.array(partition_label, dtype = object)\n",
    "for i in range(len(partition_label)):\n",
    "    if partition_label[i] == 0:\n",
    "        partition_label[i] = 'C0'\n",
    "    if partition_label[i] == 1:\n",
    "        partition_label[i] = 'C1'\n",
    "    if partition_label[i] == 2:\n",
    "        partition_label[i] = 'C2'\n",
    "lab = partition_label\n",
    "fig = plt.figure(figsize = (8,8), dpi = 300)\n",
    "ax = plt.axes(projection='3d') \n",
    "\n",
    "zline = z\n",
    "xline = x\n",
    "yline = y\n",
    "angle = 120 #120\n",
    "ax.view_init(40, angle) #20\n",
    "ax.set_xlabel('1st component')\n",
    "ax.set_ylabel('2nd component')\n",
    "ax.set_zlabel('3rd component')\n",
    "ax.scatter(xline, yline, zline, c = partition_label, s = 300)\n",
    "\n",
    "p = len(partition)\n",
    "# label of each centroid\n",
    "for i in range(len(centroids)):\n",
    "    ax.text(centroids[i,0],centroids[i,1],centroids[i,2],str(centroids_label[i]), size=30, zorder=135, color='k')\n",
    "    plt.scatter(centroids[i,0],centroids[i,1],centroids[i,2], c= 'r')\n",
    "#plt.savefig(\"/Users/endingalaporte/Desktop/3clusters_DBSCAN_withoutnoise.svg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
